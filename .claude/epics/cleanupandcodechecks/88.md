# Issue #88: Database Analysis - Schema-to-code mapping and usage scanning

**Epic**: cleanupandcodechecks
**Type**: Database Analysis Task
**Priority**: High
**Dependencies**: Issue #89 (Analysis Infrastructure)
**Dependents**: Issue #93 (Database Optimization)

## Overview

Build comprehensive schema-to-code mapping using Drizzle introspection and create database usage scanner with AST parsing to identify unused tables, columns, and indexes.

## Requirements

### 1. Schema-to-Code Mapping Tool

Build a comprehensive mapping system that:

- **Drizzle Schema Introspection**: Use Drizzle's introspection capabilities to extract complete database schema
- **Code Usage Analysis**: Parse TypeScript/JavaScript files to map schema elements to actual usage
- **Relationship Mapping**: Track foreign keys, joins, and complex relationships
- **Query Pattern Analysis**: Identify common query patterns and usage frequency

### 2. Database Usage Scanner with AST Parsing

Implement advanced AST parsing to:

- **Table Usage Detection**: Scan for direct table references in queries and ORM calls
- **Column Usage Analysis**: Track which specific columns are accessed in SELECT, WHERE, ORDER BY clauses
- **Index Usage Tracking**: Analyze query patterns to determine which indexes are actually used
- **Dynamic Query Analysis**: Handle dynamic queries and computed column references

### 3. Unused Element Identification

Create comprehensive analysis for:

- **Unused Tables**: Tables defined in schema but never referenced in code
- **Unused Columns**: Columns that exist but are never selected, filtered, or updated
- **Unused Indexes**: Indexes that don't match any query patterns in the codebase
- **Orphaned Relationships**: Foreign keys and constraints with no active usage

### 4. Database Drift Analysis

Generate reports showing:

- **Schema vs Code Drift**: Differences between database schema and actual usage patterns
- **Evolution Tracking**: How database usage has changed over time
- **Migration Impact**: Assess impact of removing unused elements
- **Performance Implications**: Size and performance benefits of cleanup

### 5. Performance Impact Assessment

Analyze and report:

- **Storage Impact**: Calculate space savings from removing unused elements
- **Query Performance**: Potential performance improvements from index cleanup
- **Maintenance Overhead**: Reduced complexity from schema simplification
- **Migration Risks**: Safety analysis for proposed removals

### 6. Integration with Existing Analysis

Leverage infrastructure from task #89:

- **Report Generation**: Use existing reporting framework for consistent output
- **Configuration Management**: Extend existing config system for database analysis
- **Tool Integration**: Integrate with knip, depcheck, and other analysis tools
- **Baseline Comparison**: Compare against baseline reports for trend analysis

## Technical Requirements

### Schema Mapping Architecture

```typescript
interface SchemaElement {
  name: string;
  type: 'table' | 'column' | 'index' | 'constraint';
  definition: string;
  dependencies: string[];
  usage: UsageInfo[];
}

interface UsageInfo {
  file: string;
  line: number;
  type: 'select' | 'insert' | 'update' | 'delete' | 'join' | 'where' | 'orderby';
  context: string;
  frequency: number;
}

interface DatabaseMapping {
  timestamp: string;
  schema: SchemaElement[];
  usage: Map<string, UsageInfo[]>;
  unused: {
    tables: string[];
    columns: string[];
    indexes: string[];
  };
  analysis: PerformanceAnalysis;
}
```

### AST Parsing Strategy

- **TypeScript Parser**: Use TypeScript compiler API for accurate AST parsing
- **Query Detection**: Identify Drizzle queries, raw SQL, and dynamic queries
- **Pattern Matching**: Recognize common ORM patterns and query builders
- **Context Analysis**: Understand query context (conditions, joins, aggregations)

### Database Introspection

- **Drizzle Integration**: Use existing Drizzle connection for schema introspection
- **Live Schema Analysis**: Compare current schema with code usage
- **Migration History**: Analyze migration files for schema evolution
- **Index Analysis**: Examine index definitions and usage patterns

## Success Criteria

1. Complete schema-to-code mapping generated
2. AST parser accurately identifies database usage
3. Unused elements correctly identified with confidence scores
4. Performance impact assessment completed
5. Integration with existing analysis infrastructure
6. Actionable recommendations for database cleanup
7. Safe migration paths for unused element removal

## Output Files

- `/analysis/scripts/schema-mapper.ts` - Schema-to-code mapping tool
- `/analysis/scripts/db-usage-scanner.ts` - Enhanced database usage scanner
- `/analysis/scripts/ast-parser.ts` - AST parsing utilities
- `/analysis/config/db-analysis.json` - Database analysis configuration
- `/analysis/reports/database/` - Database analysis reports
- Package.json script additions for database analysis

## Deliverables

### Reports Generated

1. **Schema Mapping Report** (`schema-mapping.json/md`)
2. **Usage Analysis Report** (`db-usage-analysis.json/md`)
3. **Unused Elements Report** (`unused-elements.json/md`)
4. **Performance Impact Report** (`performance-impact.json/md`)
5. **Migration Recommendations** (`migration-recommendations.json/md`)

### Analysis Scripts

1. **Schema Mapper** - Complete schema introspection and mapping
2. **Usage Scanner** - Advanced AST-based usage analysis
3. **Drift Analyzer** - Schema vs code drift detection
4. **Performance Assessor** - Impact analysis for cleanup actions

## Integration Points

- Extend existing `db-scanner.ts` from task #89
- Use report generation framework from `report-generator.ts`
- Integrate with knip configuration for dead code detection
- Leverage baseline reports for trend analysis

## Next Steps

Upon completion, enables:
- Issue #93: Database Optimization with concrete removal candidates
- Automated database cleanup with confidence scores
- Performance optimization through strategic index removal
- Safe schema evolution with usage-based decisions

## Notes

- All analysis must be non-destructive (read-only)
- Support for both development and production schema analysis
- Handle edge cases like dynamic queries and computed columns
- Provide confidence scores for removal recommendations
- Integration with existing CI/CD for continuous monitoring